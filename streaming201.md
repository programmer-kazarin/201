# Spark Streaming part 2

## Spark Streaming

* provides a way to consume continious streams of data
* scalable, FT, high-throughput,
* built on top of spark core
  * based on rdd
  * api similar to core api
* support many unputs
* integration with other spark modules
* micro batch architecture

## Streaming context

* StreamingContext is the entry point for all streaming functionality
* Wraps/extends existing Spark context    

  ```text
  val ssc = new StreamingContext(sparkcontext, Seconds(1))
  ```

2 arg = size of micro batch

```text
ssc.start()
ssc.awaitTermination()
```

`ssc.socketTextStream("localhost", 9999)` - DStream of text type

## Sources

* Basic sources
  * Packaged together with Spark Streaming library
  * Available through 
  * Socket Streams, File Streams, Actor Streams
* Advanced sources
  * Packaged separately
  * Available through utility classes
  * Kafka, Flume, Kinesis, Twitter
* Custom sources

## Operations with Dstreams

### Transformations of DStream

* results in another Dstream
* rdd api avaliable
* there are also specific dstream transformations
* lazy evaluation

### Output operations

* write data to external system
* print to console \(debug\)

### Joins

#### stream-rdd join

```text
dStream.transform(rdd => {
  rdd.join(lookTableDF)
})
```

* join stream and static table
* transform operation
* change rdd dynamically

#### stream-stream join

* inner, left, right, full
* join micro batches
* join across windows

```text
val stream1: DStream[String, String] = ...
val stream1: DStream[String, String] = ...
dStream1.join(dStream2)
```

OR

```text
val windowed1 = stream1.window(Seconds(10))
val windowed2 = stream2.window(Seconds(20))
windowed1.join(windowed2)
```

### Window operations

* window length should be divisable by micro batch length
* windows can overlap or not: configure sliding interval - moving speed of the window

  ```text
  val windowedWordCounts = pairs.reduceByKeyAndWindow(
  (a:Int,b:Int) => (a + b), 
  Seconds(5) // window length 
  , Seconds(2) // sliding interval
  ```

* `window` - Compute new DStream based on windowed batches of source DStream
* `countByWindow` - Sliding window count of elements
* `reduceByWindow` - New single element stream,Aggregating elements over a sliding interval
* `reduceByKeyAndWindow` - Applied on PairDStreams, Values for each key are aggregated over  a sliding window, Improve performance with inverse function
* `countByValueAndWindow` - Returns DStream of \(K,V\) pairs where the value of each key is it’s frequency

### Stateful operations

* pipeline of operations saving some state across multiple batches
* Operate on PairDStreams
* Fault tolerant

  Checkpoint directory must be set

* `updateStateByKey`
  * Maintain state while continuously updating it
  * In every batch updated function is applied for all existing keys

    ```text
    def updateFunction(newValues: Seq[Int], count: Option[Int]): Option[Int] = {
    Some(count.getOrElse(0) + newValues.size)
    }
    tags.map(tag => (tag,1)).updateStateByKey(updateFunction)
    ```
* `mapWithState`
  * New API
  * Optimized performance
  * Session timeouts
  * Arbitrary data can be sent to downstreams

    ```text
    val updateFunction = (key: String, value: Option[Int], 
    state: State[Int]) => {
    val newCount = value.getOrElse(0) + state.getOption.getOrElse(0L)
    state.update(newCount)
    (key, newCount)
    }
    tags.map(tag => (tag,1))
    .mapWithState(StateSpec.function(updateFunction))
    .reduceByKey{ case (x,y) => Math.max(x,y) }
    ```

## Caching

* DStreams can be persisted in memory using persist\(\) method
* Default level MEMORY\_ONLY\_SER
* Automatically happens
  * For DStreams generated by window-based or stateful operations - MEMORY\_ONLY\_SER
  * For Input DStreams that receive data over the network - MEMORY\_AND\_DISK\_SER\_2
* Cache strategy - drops out old data partitions in a least-recently-used \(LRU\) fashion
* RDD caching still accessible inside `foreachRDD` and `transform`: you can cache dstream or separate rdds

## Delivery semantics

from weakest to strongest:

* At-most once
  * Each record will be process once or not at all
  * Data loss
* At-least once
  * Each record will be process one or more times
  * Duplicates
* Exactly once
  * Each record will be process exactly once
  * No data loss and no duplication

## Fault tolerance semantics

* Receiving the data
  * Data received using Receivers or other mechanism
  * Different sources provide different guarantees
* Transforming the data
  * Processing happens with RDDs
  * RDD is fault tolerant
  * Exactly once
* Pushing out the data
  * Depends on type of output operation
  * At-least once by default
  * Exactly once can be achieved

## Semantica of received data

* File systems based
  * Fault tolerance by nature
  * Exactly once
* Receiver based
  * Unreliable receivers
    * Buffered data lost when Worker/Driver fails
    * At-most once semantics
  * Reliable receivers
    * Past data is lost only when Driver fails
    * Write-Ahead Logs introduced for zero data loss
    * At-least once semantics
* Kafka Direct API
  * Exactly once

## Kafka integration

2 ways to read from kafka:

* receiver based - classic approach
  * dstreams
  * write-ahead logs
  * at least once
  * continiously receive data using high level api
  * update offsets in Zookeeper
* receiverless
  * kafka direct streams
  * no write-ahead logs
  * faster
  * exactly once
  * read data using offset ranges in jobs using Simple API
  * query latest offsets and decide offset ranges for batch

## Checkpointing

* In case of job failures - saving intermediate results from which they are able to restore job.
* mostly needed in case of driver failure
* Resilient to failures unrelated to application logic
* Checkpoint enough information to recover from failures 
* Metadata checkpointing
  * Recover from driver failures
  * Streaming computation is stored:
    * Configuration of streaming context
    * DStream operations
    * Incomplete batches, batch states
    * periodically old batches are removed and once in several batches there is archiving of snapshots
* Data checkpointing
  * Saving generated RDDs
  * Automatic for stateful operations: Cut off long lengths of dependency chains

### configure checkpointing

```text
def functionToCreateContext(): StreamingContext = {
    val ssc = new StreamingContext(…)
    val lines = ssc. … // define streaming computations as well
    ssc.checkpoint(checkpointDirectory)
    ssc
}

val context = StreamingContext.getOrCreate(
  checkpointDirectory, functionToCreateContext _)

context. … // additional configuration irrespective of state
context.start()
context.awaitTermination()
```

## Semantics of output operations

* In case of worker failures: At-least once semantics
* Approaches to exactly once
  * Idempotent updates: Multiple attempts always write same data, saving files to file systems
  * Transactional updates: Batch time and partition index as unique identifier

```text
dstream.foreachRDD { (rdd, time) => 
   rdd.foreachPartition { partitionIterator =>
     val partitionId = TaskContext.get.partitionId
     val uniqueId = generateUniqueId(time.milliseconds, partitionId)     
     // commit data using uniqueId transactionally
}}
```

## Tips

* 1 receiver needs 1 CPU core. If not enough CPU, data will be received but not processed
* in local mode: no `local` or `local[1]` as master; use `local[n]`, n &gt; number or receivers
* cluster mode: same logic as in local; number or cores &gt; number or receivers

### foreach design patterns

* serialization issues: ser and deser will happen for each record

  ```text
  dstream.foreachRDD { rdd =>
  val connection = createNewConnection()
  rdd.foreach { record =>
    connection.send(record)
    connection.close()
  }
  }
  ```

* too many connections

  ```text
  dstream.foreachRDD { rdd =>
  rdd.foreach { record =>
    val connection = createNewConnection()
    connection.send(record)
    connection.close()
  }
  }
  ```

* intermediate approach - partitions:

  ```text
  dstream.foreachRDD { rdd =>
  rdd.foreachPartition { partitionOfRecords =>
    val connection = createNewConnection()
    partitionOfRecords.foreach(record => connection.send(record))
    connection.close()
  }}
  ```

* correct one:

  ```text
  dstream.foreachRDD { rdd =>
  rdd.foreachPartition { partitionOfRecords =>
    // static, lazily initialized pool of connections
    val connection = ConnectionPool.getConnection()
    partitionOfRecords.foreach(record => connection.send(record))
   // return to the pool for future reuse
    ConnectionPool.returnConnection(connection)
  }}
  ```

### Streaming TAB

* New tab in SparkUI in streaming mode
* Useful metrics: Counters, Statistics, Performance related properties
* Input rate - Number of incoming events and average
* Scheduling delay - Time spent until tasks being scheduled

### Performance tuning

* criteria of stable job - processing time &lt; batch interval. If it's bigger, batches appera faster than they're processed, delay is happening. Batches gather in queue. When queue is overwhelmed, app can crash
* need to monitor processing time and scheduling delay
* reduce batch processing time: 
  * change level of parallelism both for receivers and processors
  * serialization can be done
  * batch interval can be increased

### Deployment tips

* be aware of stateful operations and configure sufficient memory for executors
* package jar with dependencies
* configure checkpointing
* auto restart of driver
* configure WALs \(write ahead logs\): increase receiver parallelity, disable in-memory replication
* set maximum receiving rate
* update with graceful shutdown

